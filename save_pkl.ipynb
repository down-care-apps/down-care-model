{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved as 'trained_model.pkl'\n",
      "Accuracy: 0.9040639573617588\n",
      "Precision: 0.8938547486033519\n",
      "Recall: 0.903954802259887\n",
      "AUC: 0.9040581073090104\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91       793\n",
      "           1       0.89      0.90      0.90       708\n",
      "\n",
      "    accuracy                           0.90      1501\n",
      "   macro avg       0.90      0.90      0.90      1501\n",
      "weighted avg       0.90      0.90      0.90      1501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # Import joblib to save the model\n",
    "\n",
    "# Load the Dlib facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Paths to datasets\n",
    "down_syndrome_path = 'downSyndrome'\n",
    "healthy_path = 'healty'  # Make sure this path is correct\n",
    "\n",
    "# Constants for LBP\n",
    "RADIUS = 2\n",
    "POINTS = 8 * RADIUS\n",
    "METHOD = 'uniform'\n",
    "\n",
    "# Size of patches around landmarks\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "# List of specific landmarks to extract patches from\n",
    "landmark_indices = [\n",
    "    36, 39, 42, 45, 27, 30, 33, 31, 35, 51, 48, 54, 57, 68\n",
    "]\n",
    "\n",
    "# Define landmark pairs for geometric features\n",
    "pairs = [\n",
    "    (36, 39), (39, 42), (42, 45), (27, 30), (30, 33),\n",
    "    (33, 31), (33, 35), (30, 31), (30, 35), (33, 51),\n",
    "    (51, 48), (51, 54), (51, 57), (48, 57), (54, 57),\n",
    "    (39, 68), (42, 68)\n",
    "]\n",
    "\n",
    "# Check if the face is frontal by verifying symmetry\n",
    "def is_frontal_face(landmarks):\n",
    "    left_eye = np.mean(np.array(landmarks[36:42]), axis=0)\n",
    "    right_eye = np.mean(np.array(landmarks[42:48]), axis=0)\n",
    "    nose_tip = np.array(landmarks[30])\n",
    "    eye_distance = np.linalg.norm(left_eye - right_eye)\n",
    "    nose_to_left_eye = np.linalg.norm(nose_tip - left_eye)\n",
    "    nose_to_right_eye = np.linalg.norm(nose_tip - right_eye)\n",
    "    symmetry_threshold = 0.15 * eye_distance\n",
    "    return abs(nose_to_left_eye - nose_to_right_eye) < symmetry_threshold\n",
    "\n",
    "# Modified function to get landmarks and add the 69th point\n",
    "def get_landmarks(image_input):\n",
    "    if isinstance(image_input, str):\n",
    "        img = cv2.imread(image_input)\n",
    "    else:\n",
    "        img = image_input\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        midpoint_x = (points[21][0] + points[22][0]) // 2\n",
    "        midpoint_y = (points[21][1] + points[22][1]) // 2\n",
    "        points.append((midpoint_x, midpoint_y))\n",
    "        if is_frontal_face(points):\n",
    "            return points\n",
    "    return None\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_image(image):\n",
    "    augmented_images = []\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    augmented_images.append(flipped)\n",
    "    rows, cols = image.shape[:2]\n",
    "    for angle in [-10, 10]:\n",
    "        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "        rotated = cv2.warpAffine(image, M, (cols, rows))\n",
    "        augmented_images.append(rotated)\n",
    "    return augmented_images\n",
    "\n",
    "# Function to extract patches around specific landmarks\n",
    "def extract_patches(image, landmarks, indices, patch_size=PATCH_SIZE):\n",
    "    patches = []\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    for idx in indices:\n",
    "        (x, y) = landmarks[idx]\n",
    "        x_start = max(x - patch_size // 2, 0)\n",
    "        y_start = max(y - patch_size // 2, 0)\n",
    "        x_end = min(x + patch_size // 2, gray.shape[1])\n",
    "        y_end = min(y + patch_size // 2, gray.shape[0])\n",
    "        patch = gray[y_start:y_end, x_start:x_end]\n",
    "        if patch.size > 0:\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "# Function to extract LBP features from patches\n",
    "def extract_lbp_from_patches(patches, radius=RADIUS, points=POINTS, method=METHOD):\n",
    "    lbp_features = []\n",
    "    for patch in patches:\n",
    "        lbp = local_binary_pattern(patch, points, radius, method)\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, points + 3), range=(0, points + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-6)\n",
    "        lbp_features.extend(hist)\n",
    "    return lbp_features\n",
    "\n",
    "# Function to extract geometric features from specific landmark pairs\n",
    "def extract_geometric_features(landmarks, pairs):\n",
    "    geom_features = []\n",
    "    for i, j in pairs:\n",
    "        p1 = landmarks[i]\n",
    "        p2 = landmarks[j]\n",
    "        distance = np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "        geom_features.append(distance)\n",
    "    return geom_features\n",
    "\n",
    "# Function to extract combined features (LBP + Geometric)\n",
    "def get_combined_features(image, pairs):\n",
    "    landmarks = get_landmarks(image)\n",
    "    if landmarks:\n",
    "        patches = extract_patches(image, landmarks, landmark_indices)\n",
    "        lbp_features = extract_lbp_from_patches(patches)\n",
    "        geom_features = extract_geometric_features(landmarks, pairs)\n",
    "        combined_features = lbp_features + geom_features\n",
    "        return combined_features\n",
    "    return None\n",
    "\n",
    "# Collect data and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for path, label in [(down_syndrome_path, 1), (healthy_path, 0)]:\n",
    "    for img_file in os.listdir(path):\n",
    "        img_path = os.path.join(path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            combined_features = get_combined_features(img, pairs)\n",
    "            if combined_features:\n",
    "                X.append(combined_features)\n",
    "                y.append(label)\n",
    "                augmented_images = augment_image(img)\n",
    "                for aug_img in augmented_images:\n",
    "                    aug_features = get_combined_features(aug_img, pairs)\n",
    "                    if aug_features:\n",
    "                        X.append(aug_features)\n",
    "                        y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Handle NaNs and scale features\n",
    "X = np.nan_to_num(X)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the SVM classifier with class weights\n",
    "model = SVC(C=1, kernel='rbf', gamma='scale', probability=True, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model and scaler\n",
    "joblib.dump({'model': model, 'scaler': scaler}, 'trained_model.pkl')\n",
    "print(\"Model trained and saved as 'trained_model.pkl'\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
